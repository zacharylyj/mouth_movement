{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\zachary\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Could not understand audio\n",
      "T EH S T IH NG T EH S T\n",
      "------------------------------\n",
      "testing test ->\n",
      "['B', 'C', 'B', 'B', 'C', 'B', 'B', 'C', 'B', 'B']\n",
      "------------------------------\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "OOV\n",
      "------------------------------\n",
      "1112 ->\n",
      "['X']\n",
      "------------------------------\n",
      "Could not understand audio\n",
      "T EH S T IH NG\n",
      "------------------------------\n",
      "testing ->\n",
      "['B', 'C', 'B', 'B', 'C', 'B']\n",
      "------------------------------\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [27], line 93\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m         audio \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphrase_time_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Listen for 5 seconds\u001b[39;00m\n\u001b[0;32m     94\u001b[0m         text \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecognize_google(audio)\n\u001b[0;32m     95\u001b[0m         phonetic \u001b[38;5;241m=\u001b[39m text_to_phonetic(text)\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\azureaistudio\\lib\\site-packages\\speech_recognition\\__init__.py:523\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phrase_time_limit \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m-\u001b[39m phrase_start_time \u001b[38;5;241m>\u001b[39m phrase_time_limit:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    525\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\azureaistudio\\lib\\site-packages\\speech_recognition\\__init__.py:199\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\azureaistudio\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "mouth_shapes = {\n",
    "    'AA': 'Open mouth, tongue low',\n",
    "    'AE': 'Open mouth, tongue forward low',\n",
    "    'AH': 'Open mouth slightly, central tongue',\n",
    "    'AO': 'Rounded lips, tongue back',\n",
    "    'AW': 'Rounded lips, moving from open to closed',\n",
    "    'AY': 'Mouth wide open to slight close',\n",
    "    'B': 'Closed lips',\n",
    "    'CH': 'Teeth close, air burst',\n",
    "    'D': 'Tongue touches alveolar ridge',\n",
    "    'DH': 'Teeth close, tongue between',\n",
    "    'EH': 'Mouth open, tongue forward',\n",
    "    'ER': 'Rounded lips, tongue curled',\n",
    "    'EY': 'Mouth open, moving to close',\n",
    "    'F': 'Lower lip to upper teeth',\n",
    "    'G': 'Back of tongue against soft palate',\n",
    "    'HH': 'Open mouth, minimal tongue movement',\n",
    "    'IH': 'Slightly open mouth, tongue near front upper teeth',\n",
    "    'IY': 'Mouth slightly open, tongue high and forward',\n",
    "    'JH': 'Teeth close, tongue pushed against front teeth',\n",
    "    'K': 'Back of tongue against soft palate',\n",
    "    'L': 'Tip of tongue behind upper front teeth',\n",
    "    'M': 'Closed lips, nasal',\n",
    "    'N': 'Tongue touches alveolar ridge, nasal',\n",
    "    'NG': 'Back of tongue against soft palate, nasal',\n",
    "    'OW': 'Round lips moving from closed to slightly open',\n",
    "    'OY': 'Rounded lips, moving from open to closed',\n",
    "    'P': 'Closed lips, burst of air',\n",
    "    'R': 'Rounded lips, tongue curled back',\n",
    "    'S': 'Teeth close, tongue behind lower teeth',\n",
    "    'SH': 'Teeth close, tongue curled backward',\n",
    "    'T': 'Tongue touches alveolar ridge, burst of air',\n",
    "    'TH': 'Tongue between teeth, air passing',\n",
    "    'UH': 'Lips rounded, tongue pulled back slightly',\n",
    "    'UW': 'Lips rounded, tongue high and back',\n",
    "    'V': 'Upper teeth on lower lip',\n",
    "    'W': 'Lips rounded, relaxed',\n",
    "    'Y': 'Lips spread, tongue high',\n",
    "    'Z': 'Teeth close, tongue vibrates',\n",
    "    'ZH': 'Teeth close, tongue retracted slightly'\n",
    "}\n",
    "\n",
    "# Ensure you have the necessary resources\n",
    "nltk.download('cmudict')\n",
    "pron_dict = cmudict.dict()\n",
    "\n",
    "# Initialize the speech recognizer and microphone\n",
    "recognizer = sr.Recognizer()\n",
    "mic = sr.Microphone()\n",
    "\n",
    "def remove_numbers(string):\n",
    "    numbers_to_remove = ['0', '1', '3']\n",
    "    cleaned_string = ''.join(char for char in string if char not in numbers_to_remove)\n",
    "    return cleaned_string\n",
    "\n",
    "def text_to_phonetic(text):\n",
    "    words = text.lower().split()\n",
    "    phonetic_string =  remove_numbers(' '.join(' '.join(pron_dict[word][0]) if word in pron_dict else 'OOV' for word in words))\n",
    "    print(phonetic_string)\n",
    "    mouth_list = [\n",
    "        [\"AA\", \"D\"], [\"AE\", \"C\"], [\"AH\", \"C\"], [\"AO\", \"E\"], [\"AW\", \"E\"],\n",
    "        [\"AY\", \"E\"], [\"B\", \"A\"], [\"CH\", \"B\"], [\"D\", \"B\"], [\"DH\", \"B\"],\n",
    "        [\"EH\", \"C\"], [\"ER\", \"E\"], [\"EY\", \"E\"], [\"F\", \"G\"], [\"G\", \"B\"],\n",
    "        [\"HH\", \"H\"], [\"IH\", \"C\"], [\"IY\", \"B\"], [\"JH\", \"B\"], [\"K\", \"B\"],\n",
    "        [\"L\", \"H\"], [\"M\", \"A\"], [\"N\", \"B\"], [\"NG\", \"B\"], [\"OW\", \"F\"],\n",
    "        [\"OY\", \"F\"], [\"P\", \"A\"], [\"R\", \"B\"], [\"S\", \"B\"], [\"SH\", \"B\"],\n",
    "        [\"T\", \"B\"], [\"TH\", \"B\"], [\"UH\", \"E\"], [\"UW\", \"F\"], [\"V\", \"G\"],\n",
    "        [\"W\", \"F\"], [\"Y\", \"B\"], [\"Z\", \"B\"], [\"ZH\", \"B\"], [\"OOV\", \"X\"]\n",
    "    ]\n",
    "    mouth_movements = []\n",
    "    for phoneme in phonetic_string.split():\n",
    "        found = False\n",
    "        for pair in mouth_list:\n",
    "            if phoneme == pair[0]:\n",
    "                mouth_movements.append(pair[1])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            mouth_movements.append(\"M\")\n",
    "    return mouth_movements\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    with mic as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)  # Adjust for ambient noise once at the beginning\n",
    "        print(\"Listening...\")\n",
    "        while True:\n",
    "            try:\n",
    "                audio = recognizer.listen(source, timeout=0.1, phrase_time_limit=3.0)  # Listen for 5 seconds\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                phonetic = text_to_phonetic(text)\n",
    "                print(f\"------------------------------\\n{text} ->\\n{phonetic}\\n------------------------------\")\n",
    "            except sr.WaitTimeoutError:\n",
    "                pass\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\zachary\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Could not understand audio\n",
      "T EH S T IH NG\n",
      "------------------------------\n",
      "testing ->\n",
      "['B', 'C', 'B', 'B', 'C', 'B']\n",
      "------------------------------\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "M IH K T EH S T W AH N T UW M AY T T EH S T W AH N T UW\n",
      "------------------------------\n",
      "mic test one two might test one two ->\n",
      "['A', 'C', 'B', 'B', 'C', 'B', 'B', 'F', 'C', 'B', 'B', 'F', 'A', 'E', 'B', 'B', 'C', 'B', 'B', 'F', 'C', 'B', 'B', 'F']\n",
      "------------------------------\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "S L AY T L IY F AE S T ER\n",
      "------------------------------\n",
      "slightly faster ->\n",
      "['B', 'H', 'E', 'B', 'H', 'B', 'G', 'C', 'B', 'B', 'E']\n",
      "------------------------------\n",
      "Could not understand audio\n",
      "Could not understand audio\n",
      "Could not understand audio\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 124\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 124\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [1], line 109\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m         audio \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphrase_time_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m         text \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecognize_google(audio)\n\u001b[0;32m    111\u001b[0m         phonetic \u001b[38;5;241m=\u001b[39m text_to_phonetic(text)\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\azureaistudio\\lib\\site-packages\\speech_recognition\\__init__.py:523\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phrase_time_limit \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m-\u001b[39m phrase_start_time \u001b[38;5;241m>\u001b[39m phrase_time_limit:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    525\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\azureaistudio\\lib\\site-packages\\speech_recognition\\__init__.py:199\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\azureaistudio\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "import time\n",
    "import speech_recognition as sr\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Ensure you have the necessary resources\n",
    "nltk.download(\"cmudict\")\n",
    "pron_dict = cmudict.dict()\n",
    "\n",
    "# Initialize the speech recognizer and microphone\n",
    "recognizer = sr.Recognizer()\n",
    "mic = sr.Microphone()\n",
    "\n",
    "def remove_numbers(string):\n",
    "    numbers_to_remove = [\"0\", \"1\", \"3\"]\n",
    "    cleaned_string = \"\".join(char for char in string if char not in numbers_to_remove)\n",
    "    return cleaned_string\n",
    "\n",
    "def text_to_phonetic(text):\n",
    "    words = text.lower().split()\n",
    "    phonetic_string = remove_numbers(\n",
    "        \" \".join(\n",
    "            \" \".join(pron_dict[word][0]) if word in pron_dict else \"OOV\"\n",
    "            for word in words\n",
    "        )\n",
    "    )\n",
    "    print(phonetic_string)\n",
    "    mouth_list = [\n",
    "        [\"AA\", \"D\"], [\"AE\", \"C\"], [\"AH\", \"C\"], [\"AO\", \"E\"], [\"AW\", \"E\"],\n",
    "        [\"AY\", \"E\"], [\"B\", \"A\"], [\"CH\", \"B\"], [\"D\", \"B\"], [\"DH\", \"B\"],\n",
    "        [\"EH\", \"C\"], [\"ER\", \"E\"], [\"EY\", \"E\"], [\"F\", \"G\"], [\"G\", \"B\"],\n",
    "        [\"HH\", \"H\"], [\"IH\", \"C\"], [\"IY\", \"B\"], [\"JH\", \"B\"], [\"K\", \"B\"],\n",
    "        [\"L\", \"H\"], [\"M\", \"A\"], [\"N\", \"B\"], [\"NG\", \"B\"], [\"OW\", \"F\"],\n",
    "        [\"OY\", \"F\"], [\"P\", \"A\"], [\"R\", \"B\"], [\"S\", \"B\"], [\"SH\", \"B\"],\n",
    "        [\"T\", \"B\"], [\"TH\", \"B\"], [\"UH\", \"E\"], [\"UW\", \"F\"], [\"V\", \"G\"],\n",
    "        [\"W\", \"F\"], [\"Y\", \"B\"], [\"Z\", \"B\"], [\"ZH\", \"B\"], [\"OOV\", \"X\"],\n",
    "    ]\n",
    "    mouth_movements = []\n",
    "    for phoneme in phonetic_string.split():\n",
    "        found = False\n",
    "        for pair in mouth_list:\n",
    "            if phoneme == pair[0]:\n",
    "                mouth_movements.append(pair[1])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            mouth_movements.append(\"M\")\n",
    "    return mouth_movements\n",
    "\n",
    "# Set up the display window\n",
    "root = tk.Tk()\n",
    "\n",
    "# Define paths to your images\n",
    "image_paths = {\n",
    "    \"A\": \"mouths/group2/lisa-A.png\",\n",
    "    \"B\": \"mouths/group2/lisa-B.png\",\n",
    "    \"C\": \"mouths/group2/lisa-C.png\",\n",
    "    \"D\": \"mouths/group2/lisa-D.png\",\n",
    "    \"E\": \"mouths/group2/lisa-E.png\",\n",
    "    \"F\": \"mouths/group2/lisa-F.png\",\n",
    "    \"G\": \"mouths/group2/lisa-G.png\",\n",
    "    \"H\": \"mouths/group2/lisa-H.png\",\n",
    "    \"X\": \"mouths/group2/lisa-X.png\",\n",
    "}\n",
    "\n",
    "# Load images and prepare for display after root is initialized\n",
    "images = {shape: Image.open(path) for shape, path in image_paths.items()}\n",
    "tk_images = {shape: ImageTk.PhotoImage(image) for shape, image in images.items()}\n",
    "\n",
    "label = tk.Label(root)\n",
    "label.pack()\n",
    "\n",
    "def display_image(shape):\n",
    "    label.config(image=tk_images[shape])\n",
    "    root.update()\n",
    "    time.sleep(0.2)  # Display each shape for half a second\n",
    "\n",
    "def add_transition_shapes(phonetic):\n",
    "    # Define special transitions\n",
    "    transitions = {\n",
    "        (\"A\", \"D\"): [\"A\", \"C\", \"D\"],\n",
    "        (\"B\", \"D\"): [\"B\", \"C\", \"D\"],\n",
    "        (\"C\", \"F\"): [\"C\", \"E\", \"F\"],\n",
    "        (\"D\", \"F\"): [\"D\", \"E\", \"F\"],\n",
    "    }\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(phonetic) - 1:\n",
    "        current_shape = phonetic[i]\n",
    "        next_shape = phonetic[i + 1]\n",
    "        if (current_shape, next_shape) in transitions:\n",
    "            result.extend(transitions[(current_shape, next_shape)])\n",
    "            i += 1  # Skip the next shape because it's already included in the transition\n",
    "        else:\n",
    "            result.append(current_shape)\n",
    "        i += 1\n",
    "    if i == len(phonetic) - 1:\n",
    "        result.append(phonetic[-1])\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    with mic as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        print(\"Listening...\")\n",
    "        while True:\n",
    "            try:\n",
    "                audio = recognizer.listen(source, timeout=0.1, phrase_time_limit=3.0)\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                phonetic = text_to_phonetic(text)\n",
    "                print(f\"------------------------------\\n{text} ->\\n{phonetic}\\n------------------------------\")\n",
    "                enhanced_phonetic = add_transition_shapes(phonetic)\n",
    "                for shape in enhanced_phonetic:\n",
    "                    display_image(shape)\n",
    "            except sr.WaitTimeoutError:\n",
    "                pass\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/DanielSWolf/rhubarb-lip-sync?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display:flex; flex-wrap:wrap;'><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0001.png' style='width:100%'><br><small>mouth0001.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0002.png' style='width:100%'><br><small>mouth0002.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0003.png' style='width:100%'><br><small>mouth0003.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0004.png' style='width:100%'><br><small>mouth0004.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0005.png' style='width:100%'><br><small>mouth0005.png</small></div><br><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0006.png' style='width:100%'><br><small>mouth0006.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0007.png' style='width:100%'><br><small>mouth0007.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0008.png' style='width:100%'><br><small>mouth0008.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0009.png' style='width:100%'><br><small>mouth0009.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0010.png' style='width:100%'><br><small>mouth0010.png</small></div><br><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0011.png' style='width:100%'><br><small>mouth0011.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0012.png' style='width:100%'><br><small>mouth0012.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0013.png' style='width:100%'><br><small>mouth0013.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0014.png' style='width:100%'><br><small>mouth0014.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0015.png' style='width:100%'><br><small>mouth0015.png</small></div><br><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0016.png' style='width:100%'><br><small>mouth0016.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0017.png' style='width:100%'><br><small>mouth0017.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0018.png' style='width:100%'><br><small>mouth0018.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0019.png' style='width:100%'><br><small>mouth0019.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0020.png' style='width:100%'><br><small>mouth0020.png</small></div><br><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0021.png' style='width:100%'><br><small>mouth0021.png</small></div><div style='flex: 25%; padding: 5px; text-align: center;'><img src='mouths\\mouth0022.png' style='width:100%'><br><small>mouth0022.png</small></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import os\n",
    "def display_images_in_grid(directory, images_per_row=4):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    # Filter out only image files\n",
    "    image_files = [file for file in files if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\"))]\n",
    "    \n",
    "    # Generate HTML for displaying images in a grid\n",
    "    html_output = \"<div style='display:flex; flex-wrap:wrap;'>\"\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        image_path = os.path.join(directory, image_file)\n",
    "        html_output += \"<div style='flex: 25%; padding: 5px; text-align: center;'>\"\n",
    "        html_output += f\"<img src='{image_path}' style='width:100%'><br>\"\n",
    "        html_output += f\"<small>{image_file}</small>\"\n",
    "        html_output += \"</div>\"\n",
    "        if (i + 1) % images_per_row == 0:\n",
    "            html_output += \"<br>\"\n",
    "    html_output += \"</div>\"\n",
    "    \n",
    "    # Display HTML content\n",
    "    display(HTML(html_output))\n",
    "\n",
    "# Specify the directory containing the images\n",
    "image_directory = \"mouths\"\n",
    "\n",
    "# Call the function to display images in a grid\n",
    "display_images_in_grid(image_directory, images_per_row=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Command 's' not supported for normalization\n",
      "Warning: Command 'Z' not supported for normalization\n",
      "Warning: Command 's' not supported for normalization\n",
      "Warning: Command 'Z' not supported for normalization\n",
      "Warning: Command 'Z' not supported for normalization\n",
      "Warning: Command 'Z' not supported for normalization\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m path_data_1 \u001b[38;5;241m=\u001b[39m extract_path_data(svg_file1)\n\u001b[0;32m     86\u001b[0m path_data_2 \u001b[38;5;241m=\u001b[39m extract_path_data(svg_file2)\n\u001b[1;32m---> 87\u001b[0m \u001b[43mgenerate_interpolated_svg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_data_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_data_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minterpolated_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.svg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [8], line 76\u001b[0m, in \u001b[0;36mgenerate_interpolated_svg\u001b[1;34m(paths1, paths2, steps, output_template)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_id \u001b[38;5;129;01min\u001b[39;00m paths1\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path_id \u001b[38;5;129;01min\u001b[39;00m paths2:\n\u001b[1;32m---> 76\u001b[0m         interpolated_path \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m         dwg\u001b[38;5;241m.\u001b[39madd(dwg\u001b[38;5;241m.\u001b[39mpath(d\u001b[38;5;241m=\u001b[39minterpolated_path, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, stroke\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, stroke_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mmm))\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn [8], line 56\u001b[0m, in \u001b[0;36minterpolate_path\u001b[1;34m(path1, path2, fraction)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minterpolate_path\u001b[39m(path1, path2, fraction):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124;03m\"\"\" Interpolate between two paths at a given fraction between 0 and 1. \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     segments1 \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_commands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_svg_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     segments2 \u001b[38;5;241m=\u001b[39m normalize_commands(parse_svg_path(path2))\n\u001b[0;32m     58\u001b[0m     result \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn [8], line 35\u001b[0m, in \u001b[0;36mnormalize_commands\u001b[1;34m(segments)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m==\u001b[39m command\u001b[38;5;241m.\u001b[39mupper():  \u001b[38;5;66;03m# Convert line to cubic bezier\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m command\u001b[38;5;241m.\u001b[39mislower():  \u001b[38;5;66;03m# Relative line command\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m params\n\u001b[0;32m     36\u001b[0m         params \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, y\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, x, y]\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Absolute line command\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import svgwrite\n",
    "from svgwrite import cm, mm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_path_data(svg_file):\n",
    "    \"\"\" Extract path data from an SVG file \"\"\"\n",
    "    tree = ET.parse(svg_file)\n",
    "    root = tree.getroot()\n",
    "    path_data = {}\n",
    "    for path in root.findall('.//{http://www.w3.org/2000/svg}path'):\n",
    "        path_data[path.get('id')] = path.get('d')\n",
    "    return path_data\n",
    "\n",
    "def parse_svg_path(path):\n",
    "    \"\"\" Parse the SVG path into a list of commands and their parameters. \"\"\"\n",
    "    path_re = re.compile(r'([MmLlHhVvCcSsQqTtAaZz])\\s*([^\\sMmLlHhVvCcSsQqTtAaZz]*)')\n",
    "    segments = []\n",
    "    for command, params in path_re.findall(path):\n",
    "        params = [float(x) for x in re.findall(r'-?\\d+\\.?\\d*', params)]\n",
    "        segments.append((command, params))\n",
    "    return segments\n",
    "\n",
    "def normalize_commands(segments):\n",
    "    \"\"\" Convert line commands into equivalent cubic bezier commands for consistency. \"\"\"\n",
    "    normalized = []\n",
    "    last_x, last_y = 0, 0  # Keep track of the last x, y coordinates for relative calculations\n",
    "\n",
    "    for command, params in segments:\n",
    "        if 'M' == command.upper():  # Move command resets the last known position\n",
    "            last_x, last_y = params[-2:]\n",
    "            normalized.append((command, params))\n",
    "        elif 'L' == command.upper():  # Convert line to cubic bezier\n",
    "            if command.islower():  # Relative line command\n",
    "                x, y = params\n",
    "                params = [0, 0, x/2, y/2, x, y]\n",
    "            else:  # Absolute line command\n",
    "                x, y = params\n",
    "                params = [last_x, last_y, (last_x + x)/2, (last_y + y)/2, x, y]\n",
    "                last_x, last_y = x, y\n",
    "            new_command = 'c' if command == 'l' else 'C'\n",
    "            normalized.append((new_command, params))\n",
    "        elif 'C' == command.upper():  # Cubic Bezier command\n",
    "            last_x, last_y = params[-2:]  # Update last known position\n",
    "            normalized.append((command, params))\n",
    "        elif command.upper() in ['H', 'V', 'S', 'Q', 'T', 'A', 'Z']:  # Currently unsupported commands\n",
    "            # This needs to be handled based on specific needs or extended\n",
    "            print(f\"Warning: Command '{command}' not supported for normalization\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported command found: {command}\")\n",
    "\n",
    "    return normalized\n",
    "\n",
    "def interpolate_path(path1, path2, fraction):\n",
    "    \"\"\" Interpolate between two paths at a given fraction between 0 and 1. \"\"\"\n",
    "    segments1 = normalize_commands(parse_svg_path(path1))\n",
    "    segments2 = normalize_commands(parse_svg_path(path2))\n",
    "    result = []\n",
    "    for (command1, params1), (command2, params2) in zip(segments1, segments2):\n",
    "        if command1 != command2:\n",
    "            raise ValueError(\"Cannot interpolate between different commands after normalization\")\n",
    "        interpolated_params = [(1 - fraction) * p1 + fraction * p2 for p1, p2 in zip(params1, params2)]\n",
    "        result.append((command1, interpolated_params))\n",
    "    interpolated_path = \"\"\n",
    "    for command, params in result:\n",
    "        interpolated_path += command + ' ' + ' '.join(f'{x:.3f}' for x in params) + ' '\n",
    "    return interpolated_path.strip()\n",
    "\n",
    "def generate_interpolated_svg(paths1, paths2, steps, output_template):\n",
    "    \"\"\" Generate interpolated SVGs between path data dictionaries. \"\"\"\n",
    "    for i in range(steps + 1):\n",
    "        fraction = i / steps\n",
    "        dwg = svgwrite.Drawing(output_template.format(i), profile='tiny', size=(1920*mm, 1080*mm))\n",
    "        for path_id in paths1.keys():\n",
    "            if path_id in paths2:\n",
    "                interpolated_path = interpolate_path(paths1[path_id], paths2[path_id], fraction)\n",
    "                dwg.add(dwg.path(d=interpolated_path, fill='none', stroke='black', stroke_width=0.1*mm))\n",
    "            else:\n",
    "                print(f\"Warning: No matching path for '{path_id}' in one of the SVGs.\")\n",
    "        dwg.save()\n",
    "\n",
    "# Example use (adjust the path to your SVG files)\n",
    "svg_file1 = 'first_svg.svg'\n",
    "svg_file2 = 'second_svg.svg'\n",
    "path_data_1 = extract_path_data(svg_file1)\n",
    "path_data_2 = extract_path_data(svg_file2)\n",
    "generate_interpolated_svg(path_data_1, path_data_2, 10, 'interpolated_{}.svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAErCAYAAABDzICRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE7klEQVR4nO3WMQEAIAzAMMC/5yFjRxMFPXtnZg4AkPW2AwCAXWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOLMAADEmQEAiDMDABBnBgAgzgwAQJwZAIA4MwAAcWYAAOI+18QGUuIyDnMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "\n",
    "# Parse an SVG file and return a dictionary of path data for specified IDs\n",
    "def parse_svg(file_path, ids):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    path_data = {}\n",
    "    for path in root.findall('.//{http://www.w3.org/2000/svg}path'):\n",
    "        if path.get('id') in ids:\n",
    "            path_data[path.get('id')] = path.get('d')\n",
    "    return path_data\n",
    "\n",
    "# Function to interpolate between two path data strings (this is a placeholder)\n",
    "def interpolate_path(d1, d2, fraction):\n",
    "    # Placeholder: real interpolation logic needed\n",
    "    return d1 if fraction < 0.5 else d2\n",
    "\n",
    "# Define path IDs to animate\n",
    "path_ids = ['Inner_Mouth', 'Tongue', 'Lower_Teeth', 'Upper_teeth', 'Upper_Lips', 'Lower_Lips']\n",
    "\n",
    "# Load SVG data\n",
    "svg1_paths = parse_svg('first_svg.svg', path_ids)\n",
    "svg2_paths = parse_svg('second_svg.svg', path_ids)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim([0, 1920])\n",
    "ax.set_ylim([1080, 0])  # SVGs often have the y-axis inverted\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create a dictionary to store the patches for each path\n",
    "path_patches = {id: plt.Polygon([[0, 0]], closed=True, fc='none') for id in path_ids}\n",
    "for patch in path_patches.values():\n",
    "    ax.add_patch(patch)\n",
    "\n",
    "def update(frame):\n",
    "    fraction = frame / 50  # Animation steps\n",
    "    for id in path_ids:\n",
    "        d1 = np.array([0, 0])  # Replace these placeholders with parsed path data\n",
    "        d2 = np.array([1920, 1080])  # Replace these placeholders with parsed path data\n",
    "        new_d = interpolate_path(d1, d2, fraction)  # Simplified\n",
    "        path_patches[id].set_xy(new_d.reshape((-1, 2)))\n",
    "    return path_patches.values()\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=50, interval=50, blit=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureaistudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
